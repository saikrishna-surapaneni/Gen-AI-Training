sample RAG_APP codes 
"""
========================================================
1. RAG INGESTION PIPELINE
========================================================
Purpose:
This script loads raw text documents, splits them into
manageable chunks, converts them into vector embeddings,
and stores them inside MongoDB Atlas for semantic search.

This is the FOUNDATION of any RAG system.
If ingestion is poor → retrieval and answers will be poor.

--------------------------------------------------------
TECH STACK USED
--------------------------------------------------------
• Python 3.x
• MongoDB Atlas (Vector Search)
• HuggingFace Sentence Transformers
• LangChain (document loading + splitting)
• UTF-8 Text Encoding

--------------------------------------------------------
LIBRARIES USED & WHY
--------------------------------------------------------
dotenv
    Loads environment variables securely (.env file)

pymongo
    Connects Python to MongoDB Atlas

langchain_community.document_loaders
    Loads documents from local folders

langchain_text_splitters
    Splits large documents into smaller chunks

langchain_huggingface
    Generates embeddings using open-source models

--------------------------------------------------------
EMBEDDING MODEL
--------------------------------------------------------
Model: sentence-transformers/all-MiniLM-L6-v2
Dimension: 384
Use case: Fast, lightweight semantic search embeddings
"""

import os
from dotenv import load_dotenv
from pymongo import MongoClient
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings

# Load environment variables
load_dotenv()

HF_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")
MONGODB_URI = os.getenv("MONGODB_URI")

if not HF_TOKEN or not MONGODB_URI:
    raise EnvironmentError("Missing HuggingFace token or MongoDB URI")

# MongoDB Setup
client = MongoClient(MONGODB_URI)
db = client["rag_db"]
collection = db["test-embeddings"]

# Step 1: Load Documents
def load_documents(docs_path="docs"):
    """
    Loads all .txt files from the docs directory.
    Uses UTF-8 encoding to properly read international characters.
    """
    print("\nLoading documents...")

    if not os.path.exists(docs_path):
        raise FileNotFoundError(f"Directory '{docs_path}' does not exist")

    loader = DirectoryLoader(
        path=docs_path,
        glob="*.txt",
        loader_cls=TextLoader,
        loader_kwargs={"encoding": "utf-8", "autodetect_encoding": True}
    )

    documents = loader.load()

    if not documents:
        raise ValueError("No documents loaded")

    print(f"Loaded {len(documents)} documents")
    return documents

# Step 2: Split into Chunks
def split_documents(documents, chunk_size=1000, chunk_overlap=100):
    """
    Splits documents into smaller chunks for better retrieval accuracy.
    Overlap preserves context between chunks.
    """
    print("\nSplitting documents...")

    splitter = CharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )

    chunks = splitter.split_documents(documents)

    for i, chunk in enumerate(chunks):
        chunk.metadata["chunk_id"] = i

    print(f"Created {len(chunks)} chunks")
    return chunks

# Step 3: Store Embeddings in MongoDB
def store_embeddings(chunks):
    """
    Converts each chunk into an embedding vector and stores it in MongoDB.
    """
    print("\nGenerating embeddings using Hugging Face...")

    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    docs_to_insert = []

    for chunk in chunks:
        vector = embedding_model.embed_query(chunk.page_content)

        docs_to_insert.append({
            "text": chunk.page_content,
            "embedding": vector,
            "metadata": chunk.metadata
        })

    collection.insert_many(docs_to_insert)
    print(f"Stored {len(docs_to_insert)} embeddings in MongoDB")

# Main Pipeline
def main():
    print("\nRAG INGESTION PIPELINE (HF + MongoDB)")

    documents = load_documents()
    chunks = split_documents(documents)
    store_embeddings(chunks)

    print("\nIngestion complete!")

if __name__ == "__main__":
    main()


"""
========================================================
2. RAG RETRIEVAL PIPELINE
========================================================
Purpose:
Takes a user query, converts it into an embedding,
and retrieves the most semantically similar chunks
from MongoDB Atlas Vector Search.

This replaces traditional keyword search with
MEANING-BASED search.

--------------------------------------------------------
TECH USED
--------------------------------------------------------
• MongoDB Atlas Vector Search ($vectorSearch)
• HuggingFace Embeddings
• Cosine similarity search
"""

import os
from dotenv import load_dotenv
from pymongo import MongoClient
from langchain_huggingface import HuggingFaceEmbeddings

load_dotenv()
MONGODB_URI = os.getenv("MONGODB_URI")

if not MONGODB_URI:
    raise EnvironmentError("MONGODB_URI not found")

client = MongoClient(MONGODB_URI)
db = client["rag_db"]
collection = db["test-embeddings"]

embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

def retrieve_documents(query, k=5):
    """
    Converts query → embedding → vector search in MongoDB
    Returns top-k most relevant chunks
    """
    print(f"\nUser Query: {query}")

    query_vector = embedding_model.embed_query(query)

    pipeline = [
        {
            "$vectorSearch": {
                "index": "default",
                "path": "embedding",
                "queryVector": query_vector,
                "numCandidates": 100,
                "limit": k
            }
        },
        {
            "$project": {
                "_id": 0,
                "text": 1,
                "metadata": 1,
                "score": {"$meta": "vectorSearchScore"}
            }
        }
    ]

    return list(collection.aggregate(pipeline))

if __name__ == "__main__":
    results = retrieve_documents("How much did Microsoft pay to acquire GitHub?")
    for i, doc in enumerate(results, 1):
        print(f"\nDocument {i} | Score: {doc['score']}")
        print(doc["text"])

"""
========================================================
3. RAG ANSWER GENERATION
========================================================
Purpose:
Uses retrieved context + LLM to generate a grounded answer.

LLM Provider: Groq (LLaMA 3)
Why Groq?
• Ultra-fast inference
• Open-weight LLMs
• Cost-efficient for RAG apps
"""

import os
from dotenv import load_dotenv
from pymongo import MongoClient
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain_core.messages import SystemMessage, HumanMessage

load_dotenv()

MONGODB_URI = os.getenv("MONGODB_URI")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

client = MongoClient(MONGODB_URI)
db = client["rag_db"]
collection = db["test-embeddings"]

embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

def retrieve_context(query, k=5):
    query_vector = embedding_model.embed_query(query)

    pipeline = [
        {"$vectorSearch": {"index": "default", "path": "embedding",
                           "queryVector": query_vector, "numCandidates": 100, "limit": k}},
        {"$project": {"_id": 0, "text": 1}}
    ]

    return list(collection.aggregate(pipeline))

def answer_question(query):
    docs = retrieve_context(query)

    if not docs:
        return "I don't have enough information to answer that question."

    context = "\n\n".join(doc["text"] for doc in docs)

    prompt = f"""
Use ONLY the context below to answer the question.
If not found, say you do not know.

Context:
{context}

Question:
{query}
"""

    llm = ChatGroq(model="llama3-70b-8192", temperature=0)

    messages = [
        SystemMessage(content="You are a precise factual assistant."),
        HumanMessage(content=prompt)
    ]

    return llm.invoke(messages).content

if __name__ == "__main__":
    print(answer_question("Who owns Microsoft?"))


"""
========================================================
4. HISTORY-AWARE RAG CHATBOT
========================================================
Purpose:
Extends RAG by adding conversational memory.
Allows follow-up questions and context carry-over.

Technique Used:
Query Rewriting with LLM
"""

import os
from dotenv import load_dotenv
from pymongo import MongoClient
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

load_dotenv()

client = MongoClient(os.getenv("MONGODB_URI"))
db = client["rag_db"]
collection = db["test-embeddings"]

embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

llm = ChatGroq(model="llama3-70b-8192", temperature=0)

chat_history = []

def retrieve_docs(query, k=3):
    query_vector = embedding_model.embed_query(query)
    pipeline = [
        {"$vectorSearch": {"index": "default", "path": "embedding",
                           "queryVector": query_vector, "numCandidates": 100, "limit": k}},
        {"$project": {"_id": 0, "text": 1}}
    ]
    return list(collection.aggregate(pipeline))

def ask_question(user_question):
    if chat_history:
        rewrite_prompt = [SystemMessage(content="Rewrite question standalone.")] + chat_history + [HumanMessage(content=user_question)]
        search_query = llm.invoke(rewrite_prompt).content.strip()
    else:
        search_query = user_question

    docs = retrieve_docs(search_query)
    context = "\n\n".join(doc["text"] for doc in docs)

    final_prompt = f"""
Use ONLY the context below.

Context:
{context}

Question:
{user_question}
"""

    answer = llm.invoke([
        SystemMessage(content="You are a factual assistant."),
        HumanMessage(content=final_prompt)
    ]).content

    chat_history.append(HumanMessage(content=user_question))
    chat_history.append(AIMessage(content=answer))

    print(f"\nAnswer: {answer}")

def start_chat():
    print("History-aware RAG chatbot (type 'quit' to exit)")
    while True:
        q = input("\nYour question: ")
        if q.lower() == "quit":
            break
        ask_question(q)

if __name__ == "__main__":
    start_chat()


### Requirements.txt
# Core LangChain
langchain
langchain-community

# Hugging Face (embeddings)
langchain-huggingface
sentence-transformers
torch

# Groq (LLM for answer generation)
langchain-groq

# Database
pymongo

# Environment variables
python-dotenv
